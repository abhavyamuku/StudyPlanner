# backend/llm_adapter.py
import os
from dotenv import load_dotenv

load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise RuntimeError("GEMINI_API_KEY not found in .env")

import google.generativeai as genai
genai.configure(api_key=API_KEY)


def get_response(prompt: str, model: str = "models/gemini-flash-latest", cfg: dict = None, max_output_tokens: int = 2048) -> str:
    """
    Generate a text response from Gemini and return plain string.

    Arguments:
        prompt: str -> The text prompt for the LLM.
        model: str -> Gemini model ID to use (from YAML config).
        cfg: dict -> Optional config dict (from YAML) for temperature, etc.
        max_output_tokens: int -> Maximum tokens for output.
    """
    try:
        # Use temperature from config or default
        temperature = float(cfg.get("temperature", 0.2)) if cfg else 0.2

        # Create model object
        model_obj = genai.GenerativeModel(model)

        # Generate content
        response = model_obj.generate_content(
            prompt,
            generation_config={
                "temperature": temperature,
                "max_output_tokens": int(max_output_tokens),
            }
        )

        # 1️⃣ Try response.text
        text = getattr(response, "text", None)
        if text:
            return text.strip()

        # 2️⃣ Try candidates -> content
        if hasattr(response, "candidates") and response.candidates:
            cand = response.candidates[0]
            content = getattr(cand, "content", None)
            if isinstance(content, list):
                parts = []
                for part in content:
                    # If part is dict with text
                    if isinstance(part, dict) and "text" in part:
                        parts.append(part["text"])
                    # If part is string
                    elif isinstance(part, str):
                        parts.append(part)
                text = " ".join(parts).strip()
            else:
                text = str(cand).strip()
            if text:
                return text

        # 3️⃣ Fallback: return full response object as string
        return str(response).strip() or "No response generated by model."

    except Exception as e:
        return f"Error generating response: {e}"
